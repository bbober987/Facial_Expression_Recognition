{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Augmentation and Deeper Convolution\n",
    "\n",
    "Using 2 simple layers of convolution I achieved ~55% accuracy on the test set. To try and improve this I've done two things. First I implemented an image augmentation step. This essentially injects noise into each image sent through the model by randomly rotating or translating the image. This makes the model more robust and less succeptible to overfitting. In addition, I added in a concatenated layer of multiple filter sizes, as seen in architectures like GoogLeNet. The idea here is that we don't know which size filter is best, and maybe at a given layer, combining multiple kernel sizes is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import Sequential, Model, Input\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten, concatenate\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import keras.backend as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.DataFrame(pickle.load(open(r'D:\\Facial_Epression_Web_App\\src\\models\\data\\TrainData.p','rb')))\n",
    "traindata = traindata.sample(n = len(traindata))\n",
    "testdata = pd.DataFrame(pickle.load(open(r'D:\\Facial_Epression_Web_App\\src\\models\\data\\TestData.p','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain = traindata.iloc[:,0:2304].as_matrix(), traindata.iloc[:,2304]\n",
    "Xtest, ytest = testdata.iloc[:,0:2304].as_matrix(), testdata.iloc[:,2304]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest = Xtrain.reshape(len(Xtrain),48,48,1) , Xtest.reshape(len(Xtest),48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = to_categorical(ytrain)\n",
    "ytest = to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model needs to be built a little differently when concatenating layers. Instead of using the Sequential() object and adding layers, each layer is defined as a variable, given an input layer, and then the final model is compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_shape = (48, 48, 1)\n",
    "    kernel_sizes = [(2, 2), (3, 3), (4, 4), (5, 5)]\n",
    "    convs = []\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    for k in range(len(kernel_sizes)):#this code creates a stack of convoltions\n",
    "        conv = Conv2D(16, kernel_sizes[k], padding='same',\n",
    "                      activation='relu')(inp)\n",
    "        convs.append(conv)\n",
    "\n",
    "    concatenated = concatenate(convs, axis=1) #we concatenate them here\n",
    "    concatenated = MaxPool2D((2, 2), strides=(2, 2))(concatenated)\n",
    "    concatenated = Conv2D(64, (3, 3), activation='relu', padding='same')(concatenated)\n",
    "    concatenated = MaxPool2D((2, 2), strides=(2, 2))(concatenated)\n",
    "    concatenated = Conv2D(64, (3, 3), activation='relu', padding='same')(concatenated)\n",
    "    concatenated = MaxPool2D((2, 2), strides=(2, 2))(concatenated)\n",
    "\n",
    "    flat = Flatten()(concatenated)\n",
    "    d1 = Dense(1000, activation='relu')(flat)\n",
    "    d1 = Dropout(.3)(d1)\n",
    "    d2 = Dense(1000, activation='relu')(d1)\n",
    "    d2 = Dropout(.3)(d2)\n",
    "    d3 = Dense(500, activation='relu')(d2)\n",
    "    d3 = Dropout(.3)(d3)\n",
    "    out = Dense(7, activation='softmax')(d3)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'll make the data generator. We specify some parameters that are fairly self explanatory. This makes the images all a little different to make the model hopefully more generalizable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/31 [==============================] - 8s 242ms/step - loss: 1.0068 - categorical_accuracy: 0.6216\n",
      "Epoch 2/10\n",
      "32/31 [==============================] - 8s 243ms/step - loss: 0.9969 - categorical_accuracy: 0.6227\n",
      "Epoch 3/10\n",
      "32/31 [==============================] - 8s 236ms/step - loss: 0.9976 - categorical_accuracy: 0.6233\n",
      "Epoch 4/10\n",
      "32/31 [==============================] - 8s 238ms/step - loss: 0.9995 - categorical_accuracy: 0.6224\n",
      "Epoch 5/10\n",
      "32/31 [==============================] - 8s 242ms/step - loss: 0.9972 - categorical_accuracy: 0.6229\n",
      "Epoch 6/10\n",
      "32/31 [==============================] - 8s 242ms/step - loss: 0.9981 - categorical_accuracy: 0.6236\n",
      "Epoch 7/10\n",
      "32/31 [==============================] - 8s 240ms/step - loss: 0.9937 - categorical_accuracy: 0.6259\n",
      "Epoch 8/10\n",
      "32/31 [==============================] - 8s 242ms/step - loss: 0.9958 - categorical_accuracy: 0.6221\n",
      "Epoch 9/10\n",
      "32/31 [==============================] - 8s 237ms/step - loss: 0.9955 - categorical_accuracy: 0.6255\n",
      "Epoch 10/10\n",
      "32/31 [==============================] - 8s 244ms/step - loss: 0.9902 - categorical_accuracy: 0.6260\n"
     ]
    }
   ],
   "source": [
    "#fit command is now fit_generator. datagen is input as an argument. \n",
    "hist = mod.fit_generator(datagen.flow(\n",
    "    Xtrain, ytrain, batch_size=1000),\n",
    "                         steps_per_epoch=len(Xtrain) / 1000,\n",
    "                         epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.49      0.53       958\n",
      "          1       0.63      0.70      0.67       111\n",
      "          2       0.50      0.26      0.35      1024\n",
      "          3       0.79      0.84      0.82      1774\n",
      "          4       0.49      0.45      0.47      1247\n",
      "          5       0.69      0.78      0.73       831\n",
      "          6       0.50      0.69      0.58      1233\n",
      "\n",
      "avg / total       0.60      0.61      0.60      7178\n",
      "\n",
      "\n",
      "\n",
      "accuracy 0.61\n",
      "\n",
      "\n",
      "confusion matrix\n",
      "[[ 474   15   63   69  126   37  174]\n",
      " [  15   78    4    1    5    2    6]\n",
      " [ 121   13  271   60  237  151  171]\n",
      " [  47    1   28 1498   44   43  113]\n",
      " [ 112   11   89   98  563   26  348]\n",
      " [  21    3   51   52   19  652   33]\n",
      " [  49    2   41  111  149   35  846]]\n"
     ]
    }
   ],
   "source": [
    "preds = np.argmax(mod.predict(Xtest),axis = 1)\n",
    "print(classification_report(y_true = np.argmax(ytest,axis=1), y_pred = preds))\n",
    "print('\\n')\n",
    "print('accuracy', np.round(np.mean(preds ==  np.argmax(ytest,axis=1)),2))\n",
    "print('\\n')\n",
    "print('confusion matrix')\n",
    "print(confusion_matrix(y_true =  np.argmax(ytest,axis=1), y_pred = preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we can see some more improvements in accuracy. Furthermore we can see that the model overfits significantly less -- the training accuracy is very similar to the testing accuracy, so we can see augmentation was helpful to prevent overfitting. Of course I tested two things at once (augmentation and deeper architecture) so I can't say which is responsible for the accuracy gains here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the model building for this project at least for now. I could fiddle around with the architechture a bit, and possibly try ensembling several models to get an increase in accuracy. But I'd like to learn a bit about deploying machine learning models and web development, so I'm going to take the model here and use it to make a web app to predict the emotion of a user image taken from a webcam shot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.save_weights(r'D:\\Github\\Facial_Expression_Recognition\\src\\static\\assets\\stack_mod1.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
