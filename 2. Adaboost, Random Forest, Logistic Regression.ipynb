{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Recognition\n",
    "\n",
    "It's generally expected that neural networks are the way to go for image data. But how well do some other classification techniques work? Here I'll try some tree based methods, namely adaboost and random forest. I'll also do logistic regression. These will supply a baseline to beat for the neural networks later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = pd.DataFrame(pickle.load(open('data\\TrainData.p','rb')))\n",
    "traindata = traindata.sample(n = len(traindata))\n",
    "testdata = pd.DataFrame(pickle.load(open('data\\TestData.p','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2295</th>\n",
       "      <th>2296</th>\n",
       "      <th>2297</th>\n",
       "      <th>2298</th>\n",
       "      <th>2299</th>\n",
       "      <th>2300</th>\n",
       "      <th>2301</th>\n",
       "      <th>2302</th>\n",
       "      <th>2303</th>\n",
       "      <th>2304</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.415686</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.643137</td>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.180392</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.227451</td>\n",
       "      <td>0.498039</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15941</th>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.968627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8084</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.756863</td>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.552941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.945098</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.925490</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.956863</td>\n",
       "      <td>0.976471</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.988235</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8335</th>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.094118</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.184314</td>\n",
       "      <td>0.164706</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24243</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.145098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.992157</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.984314</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6     \\\n",
       "3194   0.356863  0.415686  0.474510  0.537255  0.592157  0.643137  0.690196   \n",
       "15941  0.980392  0.980392  0.980392  0.980392  0.980392  0.980392  0.968627   \n",
       "8084   0.862745  0.756863  0.819608  0.772549  0.721569  0.768627  0.788235   \n",
       "8335   0.313725  0.333333  0.145098  0.031373  0.019608  0.094118  0.184314   \n",
       "24243  1.000000  0.996078  1.000000  0.807843  0.313725  0.176471  0.125490   \n",
       "\n",
       "           7         8         9     ...       2295      2296      2297  \\\n",
       "3194   0.725490  0.752941  0.772549  ...   0.098039  0.231373  0.180392   \n",
       "15941  1.000000  0.741176  0.619608  ...   0.027451  0.019608  0.015686   \n",
       "8084   0.776471  0.752941  0.552941  ...   0.894118  0.945098  0.952941   \n",
       "8335   0.184314  0.164706  0.145098  ...   0.713725  0.729412  0.752941   \n",
       "24243  0.117647  0.152941  0.145098  ...   0.749020  0.733333  0.854902   \n",
       "\n",
       "           2298      2299      2300      2301      2302      2303  2304  \n",
       "3194   0.125490  0.125490  0.227451  0.498039  0.713725  0.815686   3.0  \n",
       "15941  0.007843  0.011765  0.007843  0.035294  0.062745  0.054902   0.0  \n",
       "8084   0.925490  0.901961  0.956863  0.976471  0.980392  0.988235   5.0  \n",
       "8335   0.776471  0.768627  0.768627  0.768627  0.776471  0.764706   3.0  \n",
       "24243  0.984314  0.992157  0.960784  0.949020  0.984314  0.996078   2.0  \n",
       "\n",
       "[5 rows x 2305 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31761, 2305)\n",
      "(7178, 2305)\n"
     ]
    }
   ],
   "source": [
    "print(traindata.shape)\n",
    "print(testdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain = traindata.iloc[:,0:2304], traindata.iloc[:,2304]\n",
    "Xtest, ytest = testdata.iloc[:,0:2304], testdata.iloc[:,2304]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I could do some tuning here on hyperparameters for the adaboost classifier I just want to get an idea of how well these types of models work. Sklearn makes it easy to plug and play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=500, random_state=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_mod = AdaBoostClassifier(n_estimators=500)\n",
    "adaboost_mod.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = adaboost_mod.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.23      0.13      0.17       958\n",
      "        1.0       0.21      0.27      0.23       111\n",
      "        2.0       0.20      0.09      0.12      1024\n",
      "        3.0       0.42      0.61      0.50      1774\n",
      "        4.0       0.29      0.22      0.25      1247\n",
      "        5.0       0.39      0.50      0.44       831\n",
      "        6.0       0.31      0.36      0.33      1233\n",
      "\n",
      "avg / total       0.32      0.34      0.32      7178\n",
      "\n",
      "\n",
      "\n",
      "accuracy 0.34\n",
      "\n",
      "\n",
      "confusion matrix\n",
      "[[ 125   21   71  313  144  104  180]\n",
      " [   7   30   10   24   18    5   17]\n",
      " [  86   16   92  315  146  182  187]\n",
      " [  86   28   56 1088  165  132  219]\n",
      " [ 119   19   84  371  277  114  263]\n",
      " [  31   13   70  145   49  417  106]\n",
      " [  83   18   71  351  159  111  440]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = ytest, y_pred = preds))\n",
    "print('\\n')\n",
    "print('accuracy', np.round(np.mean(preds == ytest),2))\n",
    "print('\\n')\n",
    "print('confusion matrix')\n",
    "print(confusion_matrix(y_true = ytest, y_pred = preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34% accuracy. Certainly it learned something, but we can probably do better. How about a random forest? Again I'll just go with general guidelines here. Random forests are nice because they require little optimization to work well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=48, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_mod = RandomForestClassifier(n_estimators = 500,max_features = 48)\n",
    "random_forest_mod.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = random_forest_mod.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.19      0.29       958\n",
      "        1.0       1.00      0.35      0.52       111\n",
      "        2.0       0.57      0.27      0.37      1024\n",
      "        3.0       0.44      0.82      0.57      1774\n",
      "        4.0       0.40      0.39      0.39      1247\n",
      "        5.0       0.71      0.62      0.66       831\n",
      "        6.0       0.44      0.41      0.43      1233\n",
      "\n",
      "avg / total       0.51      0.48      0.46      7178\n",
      "\n",
      "\n",
      "\n",
      "accuracy 0.48\n",
      "\n",
      "\n",
      "confusion matrix\n",
      "[[ 184    0   46  399  158   40  131]\n",
      " [   4   39    4   45    7    2   10]\n",
      " [  38    0  276  331  174   81  124]\n",
      " [  23    0   25 1447  141   43   95]\n",
      " [  29    0   59  451  482   15  211]\n",
      " [  10    0   42  144   51  514   70]\n",
      " [  18    0   31  463  184   30  507]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = ytest, y_pred = preds))\n",
    "print('\\n')\n",
    "print('accuracy', np.round(np.mean(preds == ytest),2))\n",
    "print('\\n')\n",
    "print('confusion matrix')\n",
    "print(confusion_matrix(y_true = ytest, y_pred = preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is actually pretty good. Much better than I would've expected given the adaboost results. Hpw about logistic regression? Since the dataset is large I'll just select 2000 samples and tune the regularization penalty on that. Note that I already shuffled the data when I imported it, so I can just take the first 2000 samples here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg_mod = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [.1,1,10,100]}\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_tune = GridSearchCV(logistic_reg_mod,param_grid,cv=5,scoring='f1_weighted',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [0.1, 1, 10, 100]}], pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='f1_weighted',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tune.fit(Xtrain[:2000],ytrain[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bober\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: 0.31588, std: 0.01305, params: {'C': 0.1},\n",
       " mean: 0.30439, std: 0.02534, params: {'C': 1},\n",
       " mean: 0.28887, std: 0.02149, params: {'C': 10},\n",
       " mean: 0.27940, std: 0.01824, params: {'C': 100}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tune.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg_mod = LogisticRegression(C = .1)\n",
    "logistic_reg_mod.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logistic_reg_mod.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.27      0.15      0.19       958\n",
      "        1.0       0.09      0.42      0.15       111\n",
      "        2.0       0.27      0.13      0.18      1024\n",
      "        3.0       0.45      0.67      0.54      1774\n",
      "        4.0       0.30      0.26      0.28      1247\n",
      "        5.0       0.53      0.49      0.51       831\n",
      "        6.0       0.34      0.32      0.33      1233\n",
      "\n",
      "avg / total       0.36      0.37      0.35      7178\n",
      "\n",
      "\n",
      "\n",
      "accuracy 0.37\n",
      "\n",
      "\n",
      "confusion matrix\n",
      "[[ 140   66   81  323  163   62  123]\n",
      " [   4   47    3   32   10    4   11]\n",
      " [  73   96  136  270  161  117  171]\n",
      " [  89   84   54 1196  154   54  143]\n",
      " [ 102   95   93  344  328   56  229]\n",
      " [  40   36   57  142   81  406   69]\n",
      " [  71   77   85  345  195   68  392]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true = ytest, y_pred = preds))\n",
    "print('\\n')\n",
    "print('accuracy', np.round(np.mean(preds == ytest),2))\n",
    "print('\\n')\n",
    "print('confusion matrix')\n",
    "print(confusion_matrix(y_true = ytest, y_pred = preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. Surely we should be able to beat this with a neural network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
